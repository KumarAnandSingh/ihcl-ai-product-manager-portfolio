[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "ihcl-evaluation-framework"
version = "1.0.0"
description = "Comprehensive evaluation framework for agentic AI systems in hospitality security"
authors = [
    {name = "IHCL AI Portfolio Team"},
]
license = {file = "LICENSE"}
readme = "README.md"
requires-python = ">=3.9"
keywords = ["ai", "evaluation", "testing", "hospitality", "security", "agentic"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Testing",
    "Topic :: Software Development :: Quality Assurance",
]

dependencies = [
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    "pytest-cov>=4.1.0",
    "pytest-xdist>=3.3.0",
    "numpy>=1.24.0",
    "pandas>=2.0.0",
    "scipy>=1.11.0",
    "scikit-learn>=1.3.0",
    "matplotlib>=3.7.0",
    "seaborn>=0.12.0",
    "plotly>=5.15.0",
    "dash>=2.10.0",
    "fastapi>=0.100.0",
    "uvicorn>=0.22.0",
    "pydantic>=2.0.0",
    "sqlalchemy>=2.0.0",
    "alembic>=1.11.0",
    "redis>=4.6.0",
    "celery>=5.3.0",
    "httpx>=0.24.0",
    "aiohttp>=3.8.0",
    "faker>=19.0.0",
    "pyyaml>=6.0",
    "jinja2>=3.1.0",
    "click>=8.1.0",
    "rich>=13.0.0",
    "typer>=0.9.0",
    "structlog>=23.1.0",
    "openai>=1.0.0",
    "anthropic>=0.3.0",
    "tiktoken>=0.5.0",
    "sentence-transformers>=2.2.0",
    "transformers>=4.30.0",
    "torch>=2.0.0",
    "datasets>=2.14.0",
    "evaluate>=0.4.0",
    "wandb>=0.15.0",
    "mlflow>=2.5.0",
    "prometheus-client>=0.17.0",
    "python-multipart>=0.0.6",
    "python-dotenv>=1.0.0",
    "cryptography>=41.0.0",
    "passlib>=1.7.4",
    "python-jose[cryptography]>=3.3.0",
    "bcrypt>=4.0.0",
]

[project.optional-dependencies]
dev = [
    "black>=23.0.0",
    "isort>=5.12.0",
    "flake8>=6.0.0",
    "mypy>=1.4.0",
    "pre-commit>=3.3.0",
    "pytest-mock>=3.11.0",
    "pytest-benchmark>=4.0.0",
    "coverage>=7.2.0",
    "bandit>=1.7.5",
    "safety>=2.3.0",
]

docs = [
    "mkdocs>=1.5.0",
    "mkdocs-material>=9.1.0",
    "mkdocstrings[python]>=0.22.0",
]

deployment = [
    "docker>=6.1.0",
    "kubernetes>=27.2.0",
    "helm>=3.12.0",
    "terraform>=1.5.0",
]

[project.urls]
Homepage = "https://github.com/ihcl/ai-portfolio"
Documentation = "https://ihcl-ai-portfolio.readthedocs.io/"
Repository = "https://github.com/ihcl/ai-portfolio.git"
"Bug Tracker" = "https://github.com/ihcl/ai-portfolio/issues"

[project.scripts]
eval-framework = "evaluation_framework.cli:main"
eval-benchmark = "evaluation_framework.cli:benchmark"
eval-report = "evaluation_framework.cli:report"

[tool.setuptools.packages.find]
where = ["src"]

[tool.pytest.ini_options]
minversion = "7.0"
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "--strict-markers",
    "--strict-config",
    "--verbose",
    "--tb=short",
    "--cov=src/evaluation_framework",
    "--cov-report=term-missing",
    "--cov-report=html:reports/coverage",
    "--cov-report=xml:reports/coverage.xml",
    "--junit-xml=reports/junit.xml",
    "--maxfail=1",
    "--disable-warnings",
]
markers = [
    "unit: Unit tests",
    "integration: Integration tests",
    "e2e: End-to-end tests",
    "slow: Slow running tests",
    "security: Security-related tests",
    "compliance: Compliance validation tests",
    "performance: Performance benchmarking tests",
    "hallucination: Hallucination detection tests",
    "statistical: Statistical analysis tests",
]
filterwarnings = [
    "ignore::UserWarning",
    "ignore::DeprecationWarning",
]

[tool.coverage.run]
source = ["src/evaluation_framework"]
omit = [
    "*/tests/*",
    "*/test_*",
    "*/__init__.py",
    "*/migrations/*",
    "*/venv/*",
    "*/.venv/*",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if settings.DEBUG",
    "raise AssertionError",
    "raise NotImplementedError",
    "if 0:",
    "if __name__ == .__main__.:",
    "class .*\\bProtocol\\):",
    "@(abc\\.)?abstractmethod",
]

[tool.black]
line-length = 88
target-version = ['py39', 'py310', 'py311', 'py312']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.isort]
profile = "black"
multi_line_output = 3
line_length = 88
known_first_party = ["evaluation_framework"]
known_third_party = [
    "pytest",
    "numpy",
    "pandas",
    "scipy",
    "sklearn",
    "matplotlib",
    "seaborn",
    "plotly",
    "dash",
    "fastapi",
    "pydantic",
    "sqlalchemy",
    "redis",
    "celery",
    "httpx",
    "faker",
]

[tool.mypy]
python_version = "3.9"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
show_error_codes = true

[[tool.mypy.overrides]]
module = [
    "faker.*",
    "plotly.*",
    "dash.*",
    "wandb.*",
    "mlflow.*",
    "transformers.*",
    "datasets.*",
    "evaluate.*",
    "sentence_transformers.*",
]
ignore_missing_imports = true

[tool.bandit]
exclude_dirs = ["tests", "build", "dist"]
skips = ["B101", "B601"]